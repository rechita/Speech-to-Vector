{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d97f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "c:\\Users\\rxy220002\\Anaconda3\\lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:752: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HELLO EVRY ONE WE ARE GROUPED TOO AND WE ARE GOING TO PRESENT THE ASSESSMENT OF NON NATIVE PRONUNCIATION AS OUR PROJECT REPORTED THANKS']\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# load the audio data (use your own wav file here!)\n",
    "input_audio, sr = librosa.load(r'C:\\Users\\rxy220002\\OneDrive - The University of Texas at Dallas\\Desktop\\code_spring\\ADL\\project\\input.wav', sr=16000)\n",
    "\n",
    "# tokenize\n",
    "input_values = tokenizer(input_audio, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)\n",
    "\n",
    "# print the output\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a5c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
